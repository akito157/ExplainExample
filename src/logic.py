import os
import pickle
from langchain.chat_models import ChatOpenAI
import openai
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.callbacks.base import BaseCallbackHandler
from langchain.callbacks.base import BaseCallbackManager
from langchain.callbacks.streamlit import StreamlitCallbackHandler
from typing import Any, Dict, List
import streamlit as st
import openai
import json

st.components.v1.html("""
<style>
[data-testid="stVerticalBlock"] > [data-stale="false"] > [class="stMarkdown"][style*="width:"] >[data-testid="stMarkdownContainer"] p {
    position: fixed;
    font-family: "Rounded M+ 1c medium";
    font-size: 1.5vmax;
    color: white;
    justify-content: center;
    align-items: center;
    bottom: 0%;
    left: 0%;
    padding: 1% 5% 1% 5%;
    width: 90%;
    height: 30%;
    animation: fadeOut 4s;
    overflow-y: auto;
}
</style>
""", height=0)

class SimpleStreamlitCallbackHandler(BaseCallbackHandler):
    """ Copied only streaming part from StreamlitCallbackHandler """
    
    def __init__(self) -> None:
        self.tokens_area = st.empty()
        self.tokens_stream = ""
        
    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """Run on new LLM token. Only available when streaming is enabled."""
        
        token = token.replace('\n\n', '\n')
        token = token.replace('\n', '  \n')
        
        if token==' ':
            print('space')
        if token=='\n':
            print('n')
        # if token=='\n\n':
            # print('nn')

        if len(token)>1:
            for s in token:
                self.tokens_stream += s
                self.tokens_area.markdown(self.tokens_stream)
        else:
            self.tokens_stream += token
            self.tokens_area.markdown(self.tokens_stream)
            


# å®šæ•°ã®è¨­å®š
openai.api_key = os.environ.get('OPEN_AI_KEY')

llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    # model_name="gpt-4",
    temperature=0.2,
    streaming=True

)

system_message = SystemMessage(
        content="ã‚ãªãŸã¯é€²è·¯æŒ‡å°ã®ãƒ—ãƒ­ã§ã™ã€‚é«˜æ ¡ç”Ÿã®é€²è·¯æŒ‡å°ã‚’æ‹…å½“ã—ã¦ã„ã¾ã™ã€‚å‚¾è´ã¨è‚¯å®šã‚’å¤§åˆ‡ã«ã™ã‚‹ã‚­ãƒ£ãƒªã‚¢ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚ï¼‘å¹´ç”Ÿã‹ã‚‰ï¼“å¹´ç”Ÿã®é€²è·¯ã«å¯¾ã™ã‚‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã¦ã„ã¾ã™ã€‚ç”Ÿå¾’ã«å¯¾ã—ã¦ã€è‡ªåˆ†ã‚‰ã—ãç´å¾—ã®è¡Œãã‚­ãƒ£ãƒªã‚¢ã«é€²ã‚€ãŸã‚ã«å¿…è¦ãªã“ã¨ã‚’æ•™ãˆã¦ã„ã¾ã™ã€‚ç›¸æ‰‹ã®åå‰ã‚’ã€Œã•ã‚“ã€ä»˜ã‘ã¦å‘¼ã³ã¾ã—ã‚‡ã†ã€‚æŒ¨æ‹¶ã¯çœç•¥ã—ã¦ãã ã•ã„ã€‚çµµæ–‡å­—ã‚’å¤šç”¨ã—ã¦ãã ã•ã„ã€‚"
    )

def response_logic1(name, answers):
    message = [
        system_message,
        HumanMessage(
            content=f"ç§ã®åå‰ã¯{name}ã§ã™ã€‚ç§ã¯é«˜æ ¡{answers[0]}ã§ã™ã€‚é€²è·¯ã®çŠ¶æ³ã¯ã€Œ{answers[1]}ã€ã§ã™ã€‚200æ–‡å­—ç¨‹åº¦ã§ã‚¢ãƒ‰ãƒã‚¤ã‚¹ãã ã•ã„"
        ),
    ]
    return llm(message, callbacks=[SimpleStreamlitCallbackHandler()]).content


def response_logic2(name,answers):
    message = [
        system_message,
        HumanMessage(
            content=f"ç§ã®åå‰ã¯{name}ã§ã™ã€‚ç§ã¯æœ¬ã‚’èª­ã‚€ã®ãŒ{answers[0]}ã€‚æœ¬ã‚’èª­ã‚€é »åº¦ã¯ã€Œ{answers[1]}ã€ã§ã™ã€‚200æ–‡å­—ç¨‹åº¦ã§ã‚¢ãƒ‰ãƒã‚¤ã‚¹ãã ã•ã„."
        ),
    ]
    return llm(message, callbacks=[SimpleStreamlitCallbackHandler()]).content


    
def response_logic3(name, answers):
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",
        # model_name="gpt-4",
        temperature=0.2,
        streaming=True
    )

    
    with open('static/denen_book_v2.pkl', 'rb') as f:
        db = pickle.load(f)  # å¾©å…ƒ

    functions = [
            {
                "name": "i_am_json",
                "description": "æŠ½å‡ºã•ã‚ŒãŸç‰¹å¾´ã‚’ JSON ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "new_keyword": {
                            "type": "string",
                            "description": "æ–°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                        },
                    #     "reason": {
                    #         "type": "string",
                    #         "description": "å¤‰æ›ç†ç”±",
                    # },
                },
            }
            }
    ]

    
    prompt_template1 = f"""
# æŒ‡ç¤º
ä»¥ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰é€£æƒ³ã•ã‚Œã‚‹æ„å¤–æ€§ã®ã‚ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
    
# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
{answers[0]}

# å¤‰æ›ä¾‹
## ã€Œç‰©ç†å­¦ã€ â†’ã€€ã€Œãƒ­ãƒãƒ³ã€
ã€Œç‰©ç†ã€ãŒå¥½ããªäººã¯å®‡å®™ãŒå¥½ãã§ã€Œãƒ­ãƒãƒ³ã€ã¨ã„ã†è¨€è‘‰ãŒå¥½ãã‹ã‚‚ã—ã‚Œãªã„
## ã€Œæ¥½ã—ãŸã„ã€ â†’ã€€ã€ŒFIREã€
ã€Œæ¥½ã—ãŸã„ã€äººã¯ã‚¢ãƒ¼ãƒªãƒ¼ãƒªã‚¿ã‚¤ã‚¢ã—ãŸãã¦ã€ã€ŒFIREã€ã«èˆˆå‘³ãŒã‚ã‚‹ã¯ãšã 
"""
    response = openai.ChatCompletion.create(
        model="gpt-4",
        temperature=0.5,
        messages=[
            {"role": "user", "content": prompt_template1}],
        functions=functions,
        function_call={"name": "i_am_json"},
    )
    
    message = response["choices"][0]["message"]
    new_keyword = json.loads(message["function_call"]["arguments"])['new_keyword']
    # keyword = answers[0] + "ã€" +  new_keyword['new_keyword']
    # keyword = answers[0] + "ã€" +  new_keyword['new_keyword']
    print(new_keyword)
    keyword = answers[0]

    docs = db.similarity_search_with_relevance_scores(f'ã€Œ{answers[0]}ã€', k=8)
    outputs = []
    for doc, score in docs:
        book_name = doc.page_content
        author = doc.metadata.get('è‘—è€…', 'ä¸æ˜')
        publisher = doc.metadata.get('å‡ºç‰ˆç¤¾', 'ä¸æ˜')
        location = doc.metadata.get('è«‹æ±‚è¨˜å·', 'ä¸æ˜')
        output = f'æ›¸ç±åã€Œ{book_name}ã€\nè‘—è€…:{author}ã€å‡ºç‰ˆç¤¾:{publisher}ã€é…æ¶å ´æ‰€:{location}\n'
        outputs.append(output)
    
    docs = db.similarity_search_with_relevance_scores(f'ã€Œ{new_keyword}ã€', k=4)
    for doc, score in docs:
        book_name = doc.page_content
        author = doc.metadata.get('è‘—è€…', 'ä¸æ˜')
        publisher = doc.metadata.get('å‡ºç‰ˆç¤¾', 'ä¸æ˜')
        location = doc.metadata.get('è«‹æ±‚è¨˜å·', 'ä¸æ˜')
        output = f'æ›¸ç±åã€Œ{book_name}ã€\nè‘—è€…:{author}ã€å‡ºç‰ˆç¤¾:{publisher}ã€é…æ¶å ´æ‰€:{location}\n'
        outputs.append(output)
        
    context = '\n,'.join(outputs)
    
    prompt_template = """
è²´æ–¹ã¯å›³æ›¸é¤¨ã®å¸æ›¸ã§ã™ã€‚
ä»¥ä¸‹ã®æ›¸ç±ãƒªã‚¹ãƒˆã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«è¿‘ã„æ›¸ç±ã‚’{n}å†Šå¿…ãšé¸ã‚“ã§ãã ã•ã„ã€‚
é©å½“ãªã‚‚ã®ãŒãªãã¦ã‚‚ã€å¿…ãš{n}å†Šç­”ãˆã¦ãã ã•ã„ã€‚
ãŠã™ã™ã‚ç†ç”±ã§ã¯ã€Œ{name}ã•ã‚“ã€ã¨ã€å‘¼ã‚“ã§ã‚ã’ã¾ã—ã‚‡ã†ã€‚

{context}

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {question}
æ„å¤–ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {new_keyword}

Answer in Japanese:
ã‚ãªãŸã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {question}
ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼
ğŸ“šæ›¸ç±å1ï¼šã€Œbook_nameã€
âœ’è‘—è€…ï¼šã€Œauthor_nameã€ã€ğŸ¢å‡ºç‰ˆç¤¾:ã€Œpublisherã€, ğŸ”é…æ¶å ´æ‰€ï¼šã€Œbook_placeã€
ğŸ’¡ãŠã™ã™ã‚ç†ç”±ï¼š reason
ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼
ğŸ“šæ›¸ç±å2:ã€Œbook_nameã€
âœ’è‘—è€…ï¼šã€Œauthor_nameã€ã€ğŸ¢å‡ºç‰ˆç¤¾:ã€Œpublisherã€, ğŸ”é…æ¶å ´æ‰€ï¼šã€Œbook_placeã€
ğŸ’¡ãŠã™ã™ã‚ç†ç”±ï¼š reason
ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼
æ„å¤–ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {new_keyword}
ğŸ“šæ›¸ç±å3:ã€Œbook_nameã€
âœ’è‘—è€…ï¼šã€Œauthor_nameã€ã€ğŸ¢å‡ºç‰ˆç¤¾:ã€Œpublisherã€, ğŸ”é…æ¶å ´æ‰€ï¼šã€Œbook_placeã€
ğŸ’¡ãŠã™ã™ã‚ç†ç”±ï¼š reason
    """
    # prompt_template = prompt_template.format(n=3, name=name, context=context, question="{question}")
    prompt_template = prompt_template.format(n=3, name=name, context=context, question=keyword, new_keyword=new_keyword)
    
    message = [
        HumanMessage(
            content = prompt_template
        ),
    ]
    return llm(message, callbacks=[SimpleStreamlitCallbackHandler()]).content



def response_logic4(name, answers):

    prompt_template = """
# æŒ‡ç¤ºå†…å®¹
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰å°†æ¥è€ƒãˆã‚‰ã‚Œã‚‹ä»•äº‹ã‚’ï¼•ã¤ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚æ¥­ç¨®ã¯ãƒãƒ©ãƒãƒ©ã«ã—ã¦ãã ã•ã„ã€‚

# è©³ç´°æ¡ä»¶
æœ€åˆã®ï¼“ã¤ã¯æ—¢å­˜ã«ã‚ã‚‹ä»•äº‹ã‚’ã€‚ï¼”ã¤ç›®ã¯ã€æœ€å…ˆç«¯ã®æŠ€è¡“ã‚’ä½¿ã„ã€100å¹´å¾Œã“ã‚Œã‹ã‚‰ç”Ÿã¾ã‚Œã‚‹ã‚ˆã†ãªä»•äº‹ã‚’ã€‚æœ€å¾Œã®ä¸€ã¤ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã¯å…¨ãé–¢ä¿‚ãªã„ã‘ã©ã€ç¾ä»£ã®ç¤¾ä¼šå•é¡Œã®èª²é¡Œã‚’è§£æ±ºã§ãã‚‹æ„å¤–æ€§ã®ã‚ã‚‹ä»•äº‹ã‚’ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
{keyword}

# å‡ºåŠ›å½¢å¼
ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼
ã‚ãªãŸã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}
ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼
ğŸ’¼è·æ¥­â‘ ã€Œè·æ¥­åã€
ğŸ’¡ã€æ¥­ç¨®ã€‘ï¼šXXX
ğŸ’­ã€æ¦‚è¦ã€‘ï¼šXXX
ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼
ğŸ’¼è·æ¥­â‘¡ã€Œè·æ¥­åã€
ğŸ’¡ã€æ¥­ç¨®ã€‘ï¼šXXX
ğŸ’­ã€æ¦‚è¦ã€‘ï¼šXXX
ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼
"""

    message = [
        system_message,
        HumanMessage(
            content = prompt_template.format(keyword=answers[0])
        ),
    ]
    return llm(message, callbacks=[SimpleStreamlitCallbackHandler()]).content
    
